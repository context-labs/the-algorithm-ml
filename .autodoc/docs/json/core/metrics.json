{
  "fileName": "metrics.py",
  "filePath": "core/metrics.py",
  "url": "https://github.com/twitter/the-algorithm-ml/blob/master/core/metrics.py",
  "summary": "This code provides a set of common metrics for evaluating multi-task machine learning models. These metrics are implemented as classes that inherit from various mixins and the `torchmetrics` library. The main purpose of this code is to provide a flexible way to compute evaluation metrics for multi-task models, which output predictions in the format `[task_idx, ...]`.\n\nThe `probs_and_labels` function is a utility function that extracts the probabilities and labels for a specific task from the model outputs. It is used by several metric classes to preprocess the data before computing the metric.\n\nThe following metric classes are implemented:\n\n- `Count`: Computes the count of labels for each task.\n- `Ctr`: Computes the click-through rate (CTR) for each task.\n- `Pctr`: Computes the predicted click-through rate (PCTR) for each task.\n- `Precision`: Computes the precision for each task.\n- `Recall`: Computes the recall for each task.\n- `TorchMetricsRocauc`: Computes the area under the receiver operating characteristic curve (AUROC) for each task.\n- `Auc`: Computes the area under the curve (AUC) for each task, based on a custom implementation.\n- `PosRanks`: Computes the ranks of all positive examples for each task.\n- `ReciprocalRank`: Computes the reciprocal of the ranks of all positive examples for each task.\n- `HitAtK`: Computes the fraction of positive examples that rank in the top K among their negatives for each task.\n\nThese metric classes can be used in the larger project to evaluate the performance of multi-task models on various tasks. For example, one could compute the precision and recall for each task in a multi-task classification problem:\n\n```python\nprecision = Precision()\nrecall = Recall()\n\nfor batch in data_loader:\n    outputs = model(batch)\n    precision.update(outputs)\n    recall.update(outputs)\n\nprecision_result = precision.compute()\nrecall_result = recall.compute()\n```\n\nThis would provide the precision and recall values for each task, which can be used to analyze the model's performance and make improvements.",
  "questions": "1. **Question**: What is the purpose of the `probs_and_labels` function and how does it handle multi-task models?\n   **Answer**: The `probs_and_labels` function is used to extract the probabilities and labels from the output tensor for a specific task in a multi-task model. It takes the outputs dictionary and task index as input, and returns a dictionary containing the predictions and target labels for the specified task.\n\n2. **Question**: How does the `StratifyMixin` class affect the behavior of the metrics classes in this code?\n   **Answer**: The `StratifyMixin` class provides a method `maybe_apply_stratification` that can be used to apply stratification on the outputs based on the specified keys. This mixin is inherited by the metrics classes, allowing them to apply stratification on the outputs before computing the metric values.\n\n3. **Question**: What is the purpose of the `HitAtK` class and how does it compute the metric value?\n   **Answer**: The `HitAtK` class computes the fraction of positive samples that rank in the top K among their negatives. It is essentially the precision@k metric. The class takes an integer `k` as input and computes the metric value by sorting the scores in descending order, finding the ranks of positive samples, and then calculating the fraction of positive samples with ranks less than or equal to `k`."
}
{
  "folderName": "metrics",
  "folderPath": ".autodoc/docs/json/metrics",
  "url": "https://github.com/twitter/the-algorithm-ml/tree/master/.autodoc/docs/json/metrics",
  "files": [
    {
      "fileName": "__init__.py",
      "filePath": "metrics/__init__.py",
      "url": "https://github.com/twitter/the-algorithm-ml/blob/master/metrics/__init__.py",
      "summary": "This code is responsible for importing key evaluation metrics and aggregation methods used in the `the-algorithm-ml` project. These metrics and methods are essential for assessing the performance of machine learning models and algorithms within the project.\n\nThe code imports three main components:\n\n1. **StableMean**: This is an aggregation method imported from the `aggregation` module. The `StableMean` class computes the stable mean of a given set of values. This method is useful for calculating the average performance of a model across multiple runs or datasets, ensuring that the mean is not affected by extreme values or outliers. Example usage:\n\n   ```python\n   from the_algorithm_ml.aggregation import StableMean\n\n   values = [1, 2, 3, 4, 5]\n   stable_mean = StableMean()\n   mean = stable_mean(values)\n   ```\n\n2. **AUROCWithMWU**: This is a performance metric imported from the `auroc` module. The `AUROCWithMWU` class calculates the Area Under the Receiver Operating Characteristic (AUROC) curve using the Mann-Whitney U test. This metric is widely used to evaluate the performance of binary classification models, as it measures the trade-off between true positive rate and false positive rate. Example usage:\n\n   ```python\n   from the_algorithm_ml.auroc import AUROCWithMWU\n\n   true_labels = [0, 1, 0, 1, 1]\n   predicted_scores = [0.1, 0.8, 0.3, 0.9, 0.6]\n   auroc = AUROCWithMWU()\n   score = auroc(true_labels, predicted_scores)\n   ```\n\n3. **NRCE** and **RCE**: These are performance metrics imported from the `rce` module. The `NRCE` (Normalized Relative Cross Entropy) and `RCE` (Relative Cross Entropy) classes compute the cross-entropy-based metrics for evaluating the performance of multi-class classification models. These metrics are useful for comparing the predicted probabilities of a model against the true class labels. Example usage:\n\n   ```python\n   from the_algorithm_ml.rce import NRCE, RCE\n\n   true_labels = [0, 1, 2, 1, 0]\n   predicted_probabilities = [[0.8, 0.1, 0.1], [0.2, 0.7, 0.1], [0.1, 0.2, 0.7], [0.1, 0.8, 0.1], [0.9, 0.05, 0.05]]\n   nrce = NRCE()\n   rce = RCE()\n   nrce_score = nrce(true_labels, predicted_probabilities)\n   rce_score = rce(true_labels, predicted_probabilities)\n   ```\n\nIn summary, this code imports essential evaluation metrics and aggregation methods for the `the-algorithm-ml` project, enabling users to assess the performance of their machine learning models and algorithms.",
      "questions": "1. **Question:** What is the purpose of the `# noqa` comment in the import statements?\n\n   **Answer:** The `# noqa` comment is used to tell the linter (such as flake8) to ignore the specific line for any linting errors or warnings, usually because the imported modules might not be directly used in this file but are needed for other parts of the project.\n\n2. **Question:** What are the functionalities provided by the `StableMean`, `AUROCWithMWU`, `NRCE`, and `RCE` classes?\n\n   **Answer:** These classes likely provide different algorithms or metrics for the project. `StableMean` might be an implementation of a stable mean calculation, `AUROCWithMWU` could be a version of the Area Under the Receiver Operating Characteristic curve with Mann-Whitney U test, and `NRCE` and `RCE` might be related to some form of Relative Classification Error metrics.\n\n3. **Question:** Where can I find the implementation details of these imported classes?\n\n   **Answer:** The implementation details of these classes can be found in their respective files within the same package. For example, `StableMean` can be found in the `aggregation.py` file, `AUROCWithMWU` in the `auroc.py` file, and `NRCE` and `RCE` in the `rce.py` file."
    },
    {
      "fileName": "aggregation.py",
      "filePath": "metrics/aggregation.py",
      "url": "https://github.com/twitter/the-algorithm-ml/blob/master/metrics/aggregation.py",
      "summary": "This code provides a numerically stable mean computation using the Welford algorithm. The Welford algorithm is an efficient method for calculating the mean and variance of a set of numbers, which is particularly useful when dealing with large datasets or when using lower-precision data types like float32.\n\nThe `update_mean` function takes the current mean, current weighted sum, a new value, and its weight as input arguments. It updates the mean and weighted sum using the Welford formula. This function is used to incrementally update the mean as new values are added.\n\nThe `stable_mean_dist_reduce_fn` function is used to merge the state from multiple workers. It takes a tensor with the first dimension indicating workers and returns the accumulated mean from all workers.\n\nThe `StableMean` class is a subclass of `torchmetrics.Metric` and implements the stable mean computation using the Welford algorithm. It has an `__init__` method that initializes the state with a default tensor of zeros and sets the `dist_reduce_fx` to `stable_mean_dist_reduce_fn`.\n\nThe `update` method of the `StableMean` class updates the current mean with a new value and its weight. It first checks if the weight is a tensor, and if not, converts it to a tensor. Then, it calls the `update_mean` function to update the mean and weighted sum.\n\nThe `compute` method of the `StableMean` class returns the accumulated mean.\n\nHere's an example of how to use the `StableMean` class:\n\n```python\nimport torch\nfrom the_algorithm_ml import StableMean\n\n# Create a StableMean instance\nstable_mean = StableMean()\n\n# Update the mean with new values and weights\nstable_mean.update(torch.tensor([1.0, 2.0, 3.0]), weight=torch.tensor([1.0, 1.0, 1.0]))\n\n# Compute the accumulated mean\nmean = stable_mean.compute()\nprint(mean)  # Output: tensor(2.0)\n```\n\nThis code is useful in the larger project for computing the mean of large datasets or when using lower-precision data types, ensuring that the mean calculation remains accurate and stable.",
      "questions": "1. **Question**: What is the purpose of the `StableMean` class and how does it differ from a regular mean calculation?\n   \n   **Answer**: The `StableMean` class implements a numerically stable mean computation using the Welford algorithm. This ensures that the algorithm provides a valid output even when the sum of values is larger than the maximum float32, as long as the mean is within the limit of float32. This is different from a regular mean calculation, which may not be numerically stable in such cases.\n\n2. **Question**: How does the `update_mean` function work and what is the significance of the Welford formula in this context?\n\n   **Answer**: The `update_mean` function updates the current mean and weighted sum using the Welford formula. The Welford formula is used to calculate a numerically stable mean, which is particularly useful when dealing with large sums or floating-point numbers that may cause numerical instability in regular mean calculations.\n\n3. **Question**: How does the `stable_mean_dist_reduce_fn` function handle merging the state from multiple workers?\n\n   **Answer**: The `stable_mean_dist_reduce_fn` function takes a tensor with the first dimension indicating workers, and then updates the mean and weighted sum using the `update_mean` function. This allows the function to accumulate the mean from all workers in a numerically stable manner."
    },
    {
      "fileName": "auroc.py",
      "filePath": "metrics/auroc.py",
      "url": "https://github.com/twitter/the-algorithm-ml/blob/master/metrics/auroc.py",
      "summary": "This code provides an implementation of the Area Under the Receiver Operating Characteristic (AUROC) metric using the Mann-Whitney U-test. The AUROC is a popular performance measure for binary classification problems, and this implementation is well-suited for low-CTR (Click-Through Rate) scenarios.\n\nThe main class in this code is `AUROCWithMWU`, which inherits from `torchmetrics.Metric`. It has three main methods: `__init__`, `update`, and `compute`. The `__init__` method initializes the class with a label threshold, a flag to raise an error if a class is missing, and additional keyword arguments. The `update` method appends predictions, targets, and weights to the class's internal state. The `compute` method calculates the accumulated AUROC using the `_compute_helper` function.\n\nThe `_compute_helper` function is a helper function that computes the AUROC given predictions, targets, weights, and other parameters. It sorts the predictions and targets based on their scores and true labels, calculates the weighted sum of positive and negative labels, and computes the AUROC using two different assumptions for equal predictions (weight = 1 or weight = 0). The final AUROC is calculated as the average of these two values.\n\nHere's an example of how to use the `AUROCWithMWU` class:\n\n```python\nauroc_metric = AUROCWithMWU(label_threshold=0.5, raise_missing_class=False)\n\n# Update the metric with predictions, targets, and weights\nauroc_metric.update(predictions, target, weight)\n\n# Compute the accumulated AUROC\nresult = auroc_metric.compute()\n```\n\nIn the larger project, this implementation of the AUROC metric can be used to evaluate the performance of binary classification models, especially in cases where the predicted probabilities are close to 0 and the dataset has a low click-through rate.",
      "questions": "1. **Question**: What is the purpose of the `equal_predictions_as_incorrect` parameter in the `_compute_helper` function?\n   **Answer**: The `equal_predictions_as_incorrect` parameter determines how to handle positive and negative labels with identical scores. If it is set to `True`, the function assumes that they are incorrect predictions (i.e., weight = 0). If it is set to `False`, the function assumes that they are correct predictions (i.e., weight = 1).\n\n2. **Question**: How does the `AUROCWithMWU` class handle cases where either the positive or negative class is missing?\n   **Answer**: The `AUROCWithMWU` class handles missing classes based on the `raise_missing_class` parameter. If it is set to `True`, an error will be raised if either the positive or negative class is missing. If it is set to `False`, a warning will be logged, but the computation will continue.\n\n3. **Question**: What is the purpose of the `label_threshold` parameter in the `AUROCWithMWU` class?\n   **Answer**: The `label_threshold` parameter is used to determine which labels are considered positive and which are considered negative. Labels strictly above the threshold are considered positive, while labels equal to or below the threshold are considered negative."
    },
    {
      "fileName": "rce.py",
      "filePath": "metrics/rce.py",
      "url": "https://github.com/twitter/the-algorithm-ml/blob/master/metrics/rce.py",
      "summary": "This code defines two classes, `RCE` and `NRCE`, which compute the Relative Cross Entropy (RCE) and Normalized Relative Cross Entropy (NRCE) metrics, respectively. These metrics are used for evaluating models that predict the probability of success, such as pCTR (predicted Click-Through Rate) models.\n\nThe `RCE` class computes the RCE metric by comparing the binary cross entropy of the model to a reference straw man model. The straw man model is a constant predictor, always predicting the average over the labels. The RCE is calculated as:\n\n```\nRCE(model) = 100 * (CE(reference model) - CE(model)) / CE(reference model)\n```\n\nThe `NRCE` class computes the NRCE metric by normalizing the model's predictions to match the average label seen so far. This metric can help identify the potential performance of a well-calibrated model.\n\nBoth classes inherit from `torchmetrics.Metric` and implement the `update`, `compute`, and `reset` methods. The `update` method updates the metric state with new predictions and ground truth labels, the `compute` method calculates the accumulated metric, and the `reset` method resets the metric state.\n\nThe code also provides utility functions for smoothing values (`_smooth`) and computing binary cross entropy with clipping (`_binary_cross_entropy_with_clipping`). These functions are used internally by the `RCE` and `NRCE` classes.\n\nIn the larger project, these classes can be used to evaluate the performance of machine learning models that predict probabilities, such as pCTR models in online advertising. Users can create instances of the `RCE` or `NRCE` classes and update them with model predictions and ground truth labels to compute the accumulated metric.",
      "questions": "1. **What is the purpose of the `_smooth` function?**\n\n   The `_smooth` function is used to apply label smoothing to the given values. Label smoothing is a technique used to prevent overfitting by adding a small constant to the target labels. This is done by multiplying the value by `(1.0 - label_smoothing)` and adding `0.5 * label_smoothing`.\n\n2. **What is the difference between the `RCE` and `NRCE` classes?**\n\n   The `RCE` class computes the Relative Cross Entropy metric, which measures the performance of a model predicting the probability of success compared to a reference straw man model. The `NRCE` class calculates the RCE of the normalized model, where the normalized model prediction average is normalized to the average label seen so far. The main difference is that NRCE is used to measure how good a model could potentially perform if it was well calibrated, while RCE measures the actual performance of the model.\n\n3. **How does the `update` method work in the `NRCE` class?**\n\n   The `update` method in the `NRCE` class first normalizes the predictions by applying the sigmoid function if the `nrce_from_logits` flag is set to True. Then, it applies label smoothing to the target labels and updates the mean label and mean prediction accumulators. Finally, it normalizes the predictions by multiplying them with the ratio of the mean label to the mean prediction and updates the binary cross entropy accumulator with the computed values."
    }
  ],
  "folders": [],
  "summary": "The code in the `json/metrics` folder provides essential evaluation metrics and aggregation methods for assessing the performance of machine learning models and algorithms within the `the-algorithm-ml` project. The folder contains implementations for computing the stable mean, Area Under the Receiver Operating Characteristic (AUROC) curve, and Relative Cross Entropy (RCE) metrics.\n\nThe `StableMean` class, found in `aggregation.py`, computes the stable mean of a given set of values using the Welford algorithm. This method is useful for calculating the average performance of a model across multiple runs or datasets, ensuring that the mean is not affected by extreme values or outliers. Example usage:\n\n```python\nfrom the_algorithm_ml.aggregation import StableMean\n\nvalues = [1, 2, 3, 4, 5]\nstable_mean = StableMean()\nmean = stable_mean(values)\n```\n\nThe `AUROCWithMWU` class, found in `auroc.py`, calculates the AUROC curve using the Mann-Whitney U test. This metric is widely used to evaluate the performance of binary classification models, as it measures the trade-off between true positive rate and false positive rate. Example usage:\n\n```python\nfrom the_algorithm_ml.auroc import AUROCWithMWU\n\ntrue_labels = [0, 1, 0, 1, 1]\npredicted_scores = [0.1, 0.8, 0.3, 0.9, 0.6]\nauroc = AUROCWithMWU()\nscore = auroc(true_labels, predicted_scores)\n```\n\nThe `NRCE` and `RCE` classes, found in `rce.py`, compute the cross-entropy-based metrics for evaluating the performance of multi-class classification models. These metrics are useful for comparing the predicted probabilities of a model against the true class labels. Example usage:\n\n```python\nfrom the_algorithm_ml.rce import NRCE, RCE\n\ntrue_labels = [0, 1, 2, 1, 0]\npredicted_probabilities = [[0.8, 0.1, 0.1], [0.2, 0.7, 0.1], [0.1, 0.2, 0.7], [0.1, 0.8, 0.1], [0.9, 0.05, 0.05]]\nnrce = NRCE()\nrce = RCE()\nnrce_score = nrce(true_labels, predicted_probabilities)\nrce_score = rce(true_labels, predicted_probabilities)\n```\n\nIn summary, the code in this folder enables users to assess the performance of their machine learning models and algorithms by providing essential evaluation metrics and aggregation methods. These metrics and methods can be easily integrated into the larger project, allowing developers to evaluate and compare different models and algorithms.",
  "questions": ""
}